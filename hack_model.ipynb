{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "from tensorflow import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization,SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/paperplanes/Downloads/hw_machine_learning/train 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>We all heard about Venus, the planet without a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text  score\n",
       "0  000d118  Many people have car where they live. The thin...      3\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
       "2  001ab80  People always wish they had the same technolog...      4\n",
       "3  001bdc0  We all heard about Venus, the planet without a...      4\n",
       "4  002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= train_test_split(data, test_size=0.2, random_state=42)\n",
    "Xtrain, ytrain = train['full_text'], train['score']\n",
    "Xtest, ytest = test['full_text'], test['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    "word_index = tokenizer.word_index\n",
    "#print(word_index)\n",
    "V = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train = tokenizer.texts_to_sequences(Xtrain)\n",
    "seq_test =  tokenizer.texts_to_sequences(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_list = [len(i) for i in seq_train + seq_test]\n",
    "max_len=max(seq_len_list)\n",
    "max_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\n",
    "max_seq_len = int(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) / len(seq_len_list)*100\n",
    "pad_train=pad_sequences(seq_train,truncating = 'post', padding = 'pre',maxlen=max_seq_len)\n",
    "pad_test=pad_sequences(seq_test,truncating = 'post', padding = 'pre',maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xval,ytrain,yval=train_test_split(pad_train,ytrain,\n",
    "                                             test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(Xtrain,Xval,ytrain,yval,V,D,maxlen,epochs):\n",
    "\n",
    "    print(\"----Building the model----\")\n",
    "    i = Input(shape=(maxlen,))\n",
    "    x = Embedding(V + 1, D,input_length = maxlen)(i)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv1D(32,5,activation = 'relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
    "    x = LSTM(64)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(i, x)\n",
    "    model.summary()\n",
    "\n",
    "    print(\"----Training the network----\")\n",
    "    model.compile(optimizer= Adam(0.0005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    r = model.fit(Xtrain,ytrain, \n",
    "                  validation_data = (Xval,yval), \n",
    "                  epochs = epochs, \n",
    "                  verbose = 2,\n",
    "                  batch_size = 32)\n",
    "                  #callbacks = callbacks\n",
    "    print(\"Train score:\", model.evaluate(Xtrain,ytrain))\n",
    "    print(\"Validation score:\", model.evaluate(Xval,yval))\n",
    "    n_epochs = len(r.history['loss'])\n",
    "    \n",
    "    return r,model,n_epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Building the model----\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 685)]             0         \n",
      "                                                                 \n",
      " embedding_13 (Embedding)    (None, 685, 500)          25000500  \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 685, 500)          2000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 685, 500)          0         \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 681, 32)           80032     \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 681, 32)           0         \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPooli  (None, 340, 32)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  (None, 340, 256)          164864    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " lstm_25 (LSTM)              (None, 64)                82176     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25329637 (96.62 MB)\n",
      "Trainable params: 25328637 (96.62 MB)\n",
      "Non-trainable params: 1000 (3.91 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training the network----\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paperplanes/Library/Python/3.8/lib/python/site-packages/tensorflow/python/util/dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 - 193s - loss: 0.0000e+00 - accuracy: 0.0716 - val_loss: 0.0000e+00 - val_accuracy: 0.0719 - 193s/epoch - 557ms/step\n",
      "Epoch 2/5\n",
      "347/347 - 420s - loss: 0.0000e+00 - accuracy: 0.0716 - val_loss: 0.0000e+00 - val_accuracy: 0.0719 - 420s/epoch - 1s/step\n",
      "Epoch 3/5\n",
      "347/347 - 2038s - loss: 0.0000e+00 - accuracy: 0.0716 - val_loss: 0.0000e+00 - val_accuracy: 0.0719 - 2038s/epoch - 6s/step\n",
      "Epoch 4/5\n",
      "347/347 - 2029s - loss: 0.0000e+00 - accuracy: 0.0716 - val_loss: 0.0000e+00 - val_accuracy: 0.0719 - 2029s/epoch - 6s/step\n",
      "Epoch 5/5\n",
      "347/347 - 15470s - loss: 0.0000e+00 - accuracy: 0.0716 - val_loss: 0.0000e+00 - val_accuracy: 0.0719 - 15470s/epoch - 45s/step\n",
      "347/347 [==============================] - 7893s 23s/step - loss: 0.0000e+00 - accuracy: 0.0716\n",
      "Train score: [0.0, 0.07159624248743057]\n",
      "87/87 [==============================] - 2036s 24s/step - loss: 0.0000e+00 - accuracy: 0.0719\n",
      "Validation score: [0.0, 0.07186710089445114]\n"
     ]
    }
   ],
   "source": [
    "D =500\n",
    "epochs = 5\n",
    "r,model,n_epochs = lstm_model(Xtrain,Xval,ytrain,yval,V,D,max_seq_len,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paperplanes/Library/Python/3.8/lib/python/site-packages/tensorflow/python/util/dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 197s 1s/step - loss: 0.0000e+00 - accuracy: 0.0714 - val_loss: 0.0000e+00 - val_accuracy: 0.0731\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 162s 1s/step - loss: 0.0000e+00 - accuracy: 0.0714 - val_loss: 0.0000e+00 - val_accuracy: 0.0731\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 161s 1s/step - loss: 0.0000e+00 - accuracy: 0.0714 - val_loss: 0.0000e+00 - val_accuracy: 0.0731\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 155s 993ms/step - loss: 0.0000e+00 - accuracy: 0.0714 - val_loss: 0.0000e+00 - val_accuracy: 0.0731\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=Xtrain.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(Xtrain,ytrain,epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(pad_test,ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
